---
layout: post
title: Comment j'automatise mes tests avec Pytest, Travis et Coveralls
date: 2018-06-24 21:30:00
categories: test
tags: test pytest doctest travis coveralls
image: /assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/couverture.jpg
---

Lorsque l'on cr√©e un projet en Python, on peut avoir tendance √† se focaliser sur le d√©veloppement des fonctionnalit√©s.

- J'ai un objectif,
- je r√©fl√©chis aux fonctions √† √©crire,
- je les code,
- je les teste,
- je modifie ce qui pose probl√®me,
- et je continue jusqu'√† avoir le comportement souhait√©.

Le probl√®me avec cette approche, c'est qu'en reprenant le code quelques semaines ou mois plus tard, on ne se souvient plus de certains d√©tails du contenu. On continue, on fait des √©volutions... et **CRAC**. √áa ne fonctionne plus comme avant üò≠

**D'o√π vient le probl√®me ?**

Il va falloir analyser pas-√†-pas le d√©roulement du programme, fonction apr√®s fonction, param√®tre d'entr√©e apr√®s param√®tre d'entr√©e, ligne apr√®s ligne...

Au mieux, avec un debugger ([pdb](https://docs.python.org/3/library/pdb.html) en Python). Au pire, en faisant des **print** √† des endroits strat√©giques (je le sais, je fais pareil !).

La tentation est alors grande de jeter l'√©ponge si le probl√®me est vraiment complexe.

Je vais vous expliquer dans cet article comment am√©liorer la maintenabilit√© de votre code. Ou dit diff√©remment, vous faciliter la vie !

Vous apprendrez comment ajouter des tests √† votre projet, qui seront jou√©s automatiquement lors d'une modification de votre code. Vous pourrez ainsi d√©tecter la moindre r√©gression, au plus vite.
En fin d'article, vous pourrez m√™me v√©rifier le taux de couverture de vos tests. C'est-√†-dire le pourcentage de lignes de code par lesquels passent vos tests.

# Pytest

Tester son programme revient √† √©crire des appels aux fonctions de votre programme, et indiquer la valeur attendue en sortie.

Un outil est alors utilis√© pour lire les tests, les ex√©cuter, et vous pr√©venir si le comportement attendu n'est pas le bon.

Supposons qu'on ait √©crit une fonction pour faire la somme de deux nombres :

```python
def addition(a, b):
    return a+b
```

Le test pourrait √™tre le suivant :

```python
def test_addition_2_plus_3():
    assert addition(2,3) == 5
```

`assert` est une notion qui revient souvent dans les tests logiciels. En fran√ßais, on peut le traduire par *affirmer que*. On utilise `assert` en indiquant derri√®re le test √† v√©rifier. Il doit √™tre vrai pour que le teste passe.

```python
def test_addition_4_plus_5():
    assert addition(4,5) == 10
    # Ce test va √©chouer !
```

Il existe plusieurs outils pour ex√©cuter les tests en Python.

Voici mon **TOP 3** :

- [unittest](https://docs.python.org/3/library/unittest.html) : l'outil int√©gr√© √† Python
- [nose](http://nose.readthedocs.io/en/latest/) : l'outil qui √©tend les fonctions de unittest
- [pytest](https://docs.pytest.org/en/latest/) : du m√™me type que nose, mais avec une syntaxe diff√©rente

Lors de mes recherches, **pytest** a retenu mon attention par sa syntaxe simple et sa compatibilit√© avec de nombreux outils externes (cf. Travis, que je vous pr√©sente un peu plus bas).

![Python Testing with pytest](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/livre_pytest.jpg)

Un tr√®s bon livre lui est d'ailleurs d√©di√© : [Python Testing with pytest](http://pythontesting.net/books/pytest/).

 **nose** est vraiment comparable, et il revient √† chacun de se faire sa propre id√©e. 

## Mise en place des tests

Par convention, on cr√©e √† la racine du projet un dossier `test`, dans lequel on cr√©e des fichiers de tests du type `test_*.py` (un fichier par th√®me ou module). Cela permettra √† **pytest** de les trouver tout seul.

Voici √† quoi ressemble la structure de mon projet [amazon-scraper-python](https://github.com/tducret/amazon-scraper-python/) :

```bash
.
‚îú‚îÄ‚îÄ .travis.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ MANIFEST.in
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ amazon2csv.py
‚îú‚îÄ‚îÄ amazonscraper
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ client.py
‚îú‚îÄ‚îÄ pytest.ini
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.cfg
‚îú‚îÄ‚îÄ setup.py
‚îî‚îÄ‚îÄ test
    ‚îî‚îÄ‚îÄ test_amazonscraper.py
```

On voit justement `test/test_amazonscraper.py`.

Jeton un oeil √† l'int√©rieur de **test_amazonscraper.py** :

```python
import amazonscraper

def test_amazonscraper_get_products_with_keywords():

    products = amazonscraper.search(
                                keywords="Python",
                                max_product_nb=10)

    assert len(products) == 10
```

Ce test permet de v√©rifier que l'appel √† la fonction de recherche (avec le mot-cl√© "Python" et un nombre de r√©sultats maximal de 10) nous renvoie bien 10 r√©sultats.

Pour ex√©cuter le test, on s'assure tout d'abord d'avoir install√© **pytest** :

```bash
pip install -U pytest
```

On se place ensuite dans le r√©pertoire du projet, et on lance la commande `pytest -v` (*v* est l'option *verbose* pour avoir plus de d√©tails).


![R√©sultat de la commande pytest](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/pytest_test_amazonscraper_get_products_with_keywords.png)

## Mock

*(ajout√© le 26/11/2018)*

[`Mock`](https://docs.python.org/dev/library/unittest.mock.html) est un moyen tr√®s pratique de tester des fonctions n√©cessitant des appels √† d'autres librairies ou API. Celui-ci **imite** cette fonction en la rempla√ßant par une fonction de votre choix.

Voici un exemple (r√©cup√©r√© [ici](https://rhodesmill.org/brandon/slides/2014-07-pyohio/clean-architecture/)) qui remplace la requ√™te vers un serveur web (ici une recherche de d√©finition via l'API de DuckDuckGo) par une fonction interne qui renvoie une d√©finition de notre choix.

La fonction originale √† tester :

```python
import requests

def find_definition(word):
    q = 'define ' + word
    url = 'http://api.duckduckgo.com/?'
    url += urlencode({'q': q, 'format': 'json'})
    response = requests.get(url)     # <== On va imiter cette fonction
    data = response.json()           # <== ainsi que celle-ci
    definition = data[u'Definition']
    if definition == u'':
        raise ValueError('that is not a word')
    return definition
```

Le test :

```python
from mock import patch

class FakeRequestsLibrary(object):
    def get(self, url):
        self.url = url
        return self
    def json(self):
        return self.data

def test_find_definition():
    fake = FakeRequestsLibrary()
    fake.data = {u'Definition': u'abc'}
    with patch('requests.get', fake.get):  # <== On la remplace ici
        definition = find_definition('testword')

    assert definition == 'abc'
    assert fake.url == (
        'http://api.duckduckgo.com/'
        '?q=define+testword&format=json')
```

> Cette possibilit√© de test sera particuli√®rement utile quand vous souhaiterez tester la logique de votre application sans d√©pendre d'un appel √† une API externe (qui pourrait aussi allonger la dur√©e du test).

## Doctest

En plus des tests dans un fichier d√©di√© (du type `test_*.py`), on peut √©crire des tests directement dans le code.
On les appelle les **doctest**.

Dans le commentaire de description de la fonction (appel√© √©galement *docstring*), il suffit de d√©crire l'appel √† tester (pr√©c√©d√© de `>>>`), et le retour attendu.

Reprenons l'exemple de tout √† l'heure, o√π l'on souhaite tester que la fonction **addition** est bien capable de faire `2+3 = 5` :

```python
def addition(a, b):
    """ Fonction qui additionne 2 nombres
    >>> addition(2,3)
    5
    """
    return a+b
```

C'est tr√®s pratique, car en plus de donner un exemple d'utilisation pour documenter le code, on vient de cr√©er un test automatis√©.

Il suffit alors d'appeler `pytest --doctest-modules` pour que ces doctests soient √©galement jou√©s.

On peut √©galement d√©crire des cas d'erreurs, par exemple :

```python
def addition(a, b):
    """ Fonction qui additionne 2 nombres
    >>> addition(2,3)
    5
    >>> addition("a",5)
    Traceback (most recent call last):
    ...
    TypeError: must be str, not int
    """
    return a+b
```

*(les `...` entre **Traceback** et **TypeError** permettent de ne pas recopier int√©gralement l'erreur.)*

![R√©sultat de la commande pytest avec doctest : OK](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/pytest_doctest_addition_OK.png)

Pour vous montrer ce qui se passerait en cas d'√©chec d'un test (suite √† une r√©gression par exemple), je modifie la valeur attendue pour 2+3, en mettant 6.

Voici le retour, qui indique clairement l'endroit o√π le test √©choue.

![R√©sultat de la commande pytest avec doctest : NOK](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/pytest_doctest_addition_NOK.png)

## Travis

Nous avons d√©sormais de bonnes bases pour que le code soit plus facile √† maintenir. Il suffit d'ex√©cuter r√©guli√®rement la commande **pytest** pour s'assurer que tout fonctionne correctement...

Et si on pouvait √©galement automatiser cette √©tape ü§î

![Travis CI](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/travis.png)

C'est ici qu'entre en jeu [Travis CI](https://travis-ci.org/).

Le principe de tester r√©guli√®rement le code n'est pas nouveau : on parle d'int√©gration continue.

N√©anmoins, **Travis** pr√©sente l'avantage d'√™tre tr√®s simple d'utilisation et gratuit pour les projets open source.

Dans votre projet, vous rajoutez un fichier `.travis.yml`, avec les consignes d'installation et de lancement des tests.
Voici √† quoi √ßa ressemble pour un de mes projets :

```yml
language: python
python:
  - "3.6"
# Commande pour installer votre code
install:
  - pip install .
# Commandes pour installer les d√©pendances
before_script:
  - pip install -r requirements.txt
# Commande pour ex√©cuter les tests
script:
  - pytest
```

Rendez-vous ensuite sur [le site de Travis-CI](https://travis-ci.org/) pour vous inscrire (via les identifiants de votre compte Github).

Vous pourrez alors choisir les repository des projets Github √† tester.

![S√©lection des projets √† tester sur Travis](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/travis_selection_repository.png)

Il suffit alors de faire un commit pour d√©clencher les tests sur Travis.

![Visualisation du d√©roulement des tests sur travis-ci.org](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/build_travis.png)

Et si jamais cela ne se passait pas comme pr√©vu, vous recevez un petit mail :

![Exemple de mail envoy√© par Travis quand une modification de code provoque une erreur](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/mail_travis_build_broken.png)

Ce n'est qu'un aper√ßu, les possibilit√©s sont tr√®s nombreuses :

- plusieurs types de serveurs de tests
- plusieurs versions de Python
- d√©p√¥t du package g√©n√©r√© sur Pypi
- d√©ploiement automatique
- envoi de notifications (email, IRC, Slack, webhooks...)

Plus d'infos sur [la documentation Travis](https://docs.travis-ci.com/user/getting-started/)

## Coveralls

Un dernier point √† regarder de pr√®s quand on fait des tests : la couverture de code.
Il s'agit d'analyser par quelles lignes passent vos tests... et par quelles lignes ils ne passent pas.
On fait alors un rapport entre le nombre de lignes *couvertes* et le nombre de lignes total.
On obtient un pourcentage : le taux de couverture de code, qui donne une indication sur la qualit√© des tests sur le projet.
Un projet avec 50% de taux de couverture de code donne une mauvaise image, car les tests mis en place passent seulement par la moiti√© des lignes de code. On risque de garder de nombreux morceaux de code obsol√®tes, sans le savoir, jusqu'au jour o√π...

On peut faire cette analyse tr√®s facilement avec **pytest** et le plugin **pytest-cov**.

```bash
pip install pytest-cov
```

Exemple d'utilisation : 

```bash
pytest --cov=myproj tests/

-------------------- coverage: ... ---------------------
Name                 Stmts   Miss  Cover
----------------------------------------
myproj/__init__          2      0   100%
myproj/myproj          257     13    94%
myproj/feature4286      94      7    92%
----------------------------------------
TOTAL                  353     20    94%
```

Pas mal du tout, mais on peut faire mieux !

![Coveralls.io](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/coveralls.png)

Un service existe sur le web pour faciliter cette analyse : [Coveralls](http://coveralls.io/)

C'est √©galement gratuit pour les projets open source, et on peut s'inscrire avec son compte Github.

En connectant Coveralls √† Travis, on peut g√©n√©rer l'analyse de couverture de code automatiquement.

Pour cela, on ajoute simplement quelques lignes de plus dans `.travis.yml`

```yml
language: python
python:
  - "3.6"
# Commande pour installer votre code
install:
  - pip install .
# Commandes pour installer les d√©pendances
before_script:
  - pip install -r requirements.txt
  - pip install python-coveralls
  - pip install pytest-cov
# Commande pour ex√©cuter les tests
script:
  - pytest
# Commande pour envoyer les r√©sultats de couverture de code √† coveralls.io
after_success:
  coveralls
```

J'ai √©galement ajout√© un fichier `pytest.ini` au m√™me endroit, pour que pytest puisse r√©cup√©rer ses param√®tres d'appel :

```
[pytest]
addopts = --doctest-modules --cov amazonscraper
```

Apr√®s chaque commit sur Github, Travis va installer le code, ex√©cuter tous mes tests, et envoyer la couverture de code √† Coveralls. Je peux alors connaitre mon taux de couverture sur Coveralls, et analyser les lignes non couvertes :

![Lignes non couvertes sur Coveralls](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/coveralls_lignes_non_couvertes.png)

Dans mon cas, il s'agit d'un cas bien particulier : la gestion d'une exception en cas d'√©chec de communication SSL avec les serveurs d'Amazon. C'est donc difficile √† tester de mani√®re automatique (car √ßa n'est pas syst√©matique).
On peut n√©anmoins simuler ce comportement pour les tests √† l'aide de [**Mock**](#mock).

![Badges sur Github](/assets/article_images/2018-06-24-comment-j-automatise-mes-tests-avec-pytest-travis-et-coveralls/badges_github.png)

**BONUS** : Vous pouvez alors afficher fi√®rement l'√©tat des tests et la couverture de votre code, sur la page Github de votre projet. Les badges seront mis √† jour dynamiquement √† chaque modification de votre programme.

![Test successful](https://media.giphy.com/media/vq4q4LqJv3Qcg/giphy.gif)

Si vous ne voulez manquer aucun article, soyez notifi√© directement dans votre boite mail [en vous inscrivant √† la newsletter](http://bit.ly/newsletter-tducret)